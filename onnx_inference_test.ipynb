{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236b1284",
   "metadata": {},
   "source": [
    "# ONNX model session tests (quantized models)\n",
    "\n",
    "This notebook tests individual ONNX sessions (preprocessor, encoder, decoder_joint) using small synthetic inputs and runs each inference in a subprocess with a timeout to detect hangs. Edit the model paths below to point to your quantized encoder, decoder_joint and preprocessor ONNX files.\n",
    "\n",
    "Workflow:\n",
    "1. Print available providers\n",
    "2. Inspect model inputs/outputs\n",
    "3. Build small dummy inputs (replace dynamic dims with small constants)\n",
    "4. Run each model in a subprocess with timeout and print status (ok/error/timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c465f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and helpers\n",
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from pathlib import Path\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def available_providers():\n",
    "    return onnxruntime.get_available_providers()\n",
    "\n",
    "def _small_constant_for_dim(name, idx):\n",
    "    # heuristics for picking small sizes for dynamic dims\n",
    "    lname = name.lower()\n",
    "    if 'length' in lname or 'len' in lname:\n",
    "        return 1\n",
    "    if 'signal' in lname or 'audio' in lname or 'samples' in lname:\n",
    "        return 16000  # one second at 16k\n",
    "    # common feature dims\n",
    "    if idx == 1:\n",
    "        return 257\n",
    "    # time axis fallback\n",
    "    return 10\n",
    "\n",
    "def build_dummy_inputs_from_session(sess):\n",
    "    inputs = {}\n",
    "    for inp in sess.get_inputs():\n",
    "        shape = inp.shape\n",
    "        name = inp.name\n",
    "        arr_shape = []\n",
    "        for i, dim in enumerate(shape):\n",
    "            if dim is None:\n",
    "                arr_shape.append(_small_constant_for_dim(name, i))\n",
    "            else:\n",
    "                arr_shape.append(int(dim))\n",
    "        dtype = np.float32 if 'float' in inp.type.lower() or 'tensor(float' in inp.type.lower() else np.int64\n",
    "        if dtype == np.float32:\n",
    "            inputs[name] = np.random.randn(*arr_shape).astype(np.float32)\n",
    "        else:\n",
    "            # lengths/indices\n",
    "            inputs[name] = np.ones(arr_shape, dtype=np.int64)\n",
    "    return inputs\n",
    "\n",
    "def run_model_in_subprocess(model_path, input_dict, providers=None, timeout=10, use_serialized=False):\n",
    "    \"\"\"\n",
    "    Load the model inside a subprocess and run session.run to detect hangs.\n",
    "    Returns dict: {status: 'ok'|'error'|'timeout', result: ...}\n",
    "    \"\"\"\n",
    "    def worker(q, model_path, input_dict, providers, use_serialized):\n",
    "        try:\n",
    "            import onnxruntime, onnx\n",
    "            sess_options = onnxruntime.SessionOptions()\n",
    "            sess_options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "            # prefer file path when possible; some quantized models work via file\n",
    "            if use_serialized:\n",
    "                m = onnx.load(model_path)\n",
    "                sess = onnxruntime.InferenceSession(m.SerializeToString(), providers=providers, sess_options=sess_options)\n",
    "            else:\n",
    "                sess = onnxruntime.InferenceSession(str(model_path), providers=providers, sess_options=sess_options)\n",
    "            start = time.time()\n",
    "            out = sess.run(None, input_dict)\n",
    "            duration = time.time() - start\n",
    "            # return lightweight metadata (shapes)\n",
    "            out_meta = [{\n",
    "                'idx': i,\n",
    "                'shape': getattr(o, 'shape', np.array(o).shape),\n",
    "                'dtype': str(type(o))\n",
    "            } for i, o in enumerate(out)]\n",
    "            q.put(('ok', {'duration': duration, 'out_meta': out_meta}))\n",
    "        except Exception as e:\n",
    "            q.put(('err', repr(e)))\n",
    "\n",
    "    q = Queue()\n",
    "    p = Process(target=worker, args=(q, str(model_path), input_dict, providers, use_serialized))\n",
    "    p.start()\n",
    "    p.join(timeout)\n",
    "    if p.is_alive():\n",
    "        p.terminate()\n",
    "        return {'status': 'timeout'}\n",
    "    if q.empty():\n",
    "        return {'status': 'no_result'}\n",
    "    status, payload = q.get()\n",
    "    if status == 'ok':\n",
    "        return {'status': 'ok', 'result': payload}\n",
    "    return {'status': 'error', 'error': payload}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6431813c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available providers: ['AzureExecutionProvider', 'CPUExecutionProvider']\n",
      "Preprocessor exists? True\n",
      "Encoder exists? True\n",
      "Decoder-joint exists? True\n"
     ]
    }
   ],
   "source": [
    "# Configure your model paths here\n",
    "BASE = Path('.')\n",
    "preprocessor_path = BASE / 'models' / 'preprocessor-stt_de_fastconformer_hybrid_large_pc.onnx'\n",
    "encoder_path = BASE / 'quantized_models' / 'encoder-stt_de_fastconformer_hybrid_large_pc_qint8_not_per_channel.onnx'\n",
    "decoder_joint_path = BASE / 'models' / 'decoder_joint-stt_de_fastconformer_hybrid_large_pc.onnx'\n",
    "\n",
    "print('Available providers:', available_providers())\n",
    "print('Preprocessor exists?', preprocessor_path.exists())\n",
    "print('Encoder exists?', encoder_path.exists())\n",
    "print('Decoder-joint exists?', decoder_joint_path.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81b669b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor inputs:\n",
      "  input_signal ['input_signal_dynamic_axes_1', 'input_signal_dynamic_axes_2'] tensor(float)\n",
      "  length ['length_dynamic_axes_1'] tensor(int64)\n",
      "Preprocessor outputs:\n",
      "  processed_signal ['Castprocessed_signal_dim_0', 80, 'Castprocessed_signal_dim_2'] tensor(float)\n",
      "  processed_length ['length_dynamic_axes_1'] tensor(int64)\n",
      "Failed to inspect preprocessor session: ValueError(\"invalid literal for int() with base 10: 'input_signal_dynamic_axes_1'\")\n"
     ]
    }
   ],
   "source": [
    "# Test preprocessor model\n",
    "if preprocessor_path.exists():\n",
    "    try:\n",
    "        sess = onnxruntime.InferenceSession(str(preprocessor_path))\n",
    "        print('Preprocessor inputs:')\n",
    "        for inp in sess.get_inputs():\n",
    "            print(' ', inp.name, inp.shape, inp.type)\n",
    "        print('Preprocessor outputs:')\n",
    "        for out in sess.get_outputs():\n",
    "            print(' ', out.name, out.shape, out.type)\n",
    "\n",
    "        dummy = build_dummy_inputs_from_session(sess)\n",
    "        print('Running preprocessor in subprocess with timeout=10s...')\n",
    "        res = run_model_in_subprocess(preprocessor_path, dummy, providers=None, timeout=10)\n",
    "        print('Preprocessor result:', res)\n",
    "    except Exception as e:\n",
    "        print('Failed to inspect preprocessor session:', repr(e))\n",
    "else:\n",
    "    print('Preprocessor model file not found; update preprocessor_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec9a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to inspect encoder session: NotImplemented(\"[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/pre_encode/conv/conv.0/Conv_quant'\")\n"
     ]
    }
   ],
   "source": [
    "# Test encoder model\n",
    "if encoder_path.exists():\n",
    "    try:\n",
    "        sess_enc = onnxruntime.InferenceSession(str(encoder_path))\n",
    "        print('Encoder inputs:')\n",
    "        for inp in sess_enc.get_inputs():\n",
    "            print(' ', inp.name, inp.shape, inp.type)\n",
    "        print('Encoder outputs:')\n",
    "        for out in sess_enc.get_outputs():\n",
    "            print(' ', out.name, out.shape, out.type)\n",
    "\n",
    "        enc_dummy = build_dummy_inputs_from_session(sess_enc)\n",
    "        # If preprocessor produced features, you would feed them here; we just test encoder standalone\n",
    "        print('Running encoder in subprocess with timeout=10s...')\n",
    "        res_enc = run_model_in_subprocess(encoder_path, enc_dummy, providers=None, timeout=10)\n",
    "        print('Encoder result:', res_enc)\n",
    "    except Exception as e:\n",
    "        print('Failed to inspect encoder session:', repr(e))\n",
    "else:\n",
    "    print('Encoder model file not found; update encoder_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c94ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test decoder_joint model (single-step)\n",
    "if decoder_joint_path.exists():\n",
    "    try:\n",
    "        sess_dec = onnxruntime.InferenceSession(str(decoder_joint_path))\n",
    "        print('Decoder-joint inputs:')\n",
    "        for inp in sess_dec.get_inputs():\n",
    "            print(' ', inp.name, inp.shape, inp.type)\n",
    "        print('Decoder-joint outputs:')\n",
    "        for out in sess_dec.get_outputs():\n",
    "            print(' ', out.name, out.shape, out.type)\n",
    "\n",
    "        dec_dummy = build_dummy_inputs_from_session(sess_dec)\n",
    "        # Ensure common names for targets/length exist: if not, create minimal ones\n",
    "        names = [i.name for i in sess_dec.get_inputs()]\n",
    "        if any('targets' in n.lower() for n in names) and 'targets' not in dec_dummy:\n",
    "            # try to fill a target placeholder\n",
    "            first_input = sess_dec.get_inputs()[0]\n",
    "            batch = 1\n",
    "            dec_dummy.setdefault('targets', np.zeros((batch,1), dtype=np.int32))\n",
    "        print('Running decoder-joint in subprocess with timeout=10s...')\n",
    "        res_dec = run_model_in_subprocess(decoder_joint_path, dec_dummy, providers=None, timeout=10)\n",
    "        print('Decoder-joint result:', res_dec)\n",
    "    except Exception as e:\n",
    "        print('Failed to inspect decoder-joint session:', repr(e))\n",
    "else:\n",
    "    print('Decoder-joint model file not found; update decoder_joint_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a52c7",
   "metadata": {},
   "source": [
    "Next steps / troubleshooting tips:\n",
    "- If one model times out, try re-running that model with different providers: CPU first (['CPUExecutionProvider']) and, if available, GPU providers.\n",
    "- Try toggling session options: lower graph_optimization_level or disable optimizations to see if quantized ops cause problems.\n",
    "- If a subprocess shows timeout but no exception, try running the same session.run in a fresh Python script (outside notebook) to reproduce.\n",
    "- If preprocessor runs but encoder hangs, feed the produced preprocessor outputs to encoder to confirm exact input shapes.\n",
    "- Enable ORT profiling in a separate invocation: sess_options.enable_profiling = True to capture runtime traces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
